一个系统，维护id->value的信息，信息量巨大，需多台服务器存储。系统可支持信息的增、删、改、查询及批量查询，每日增删该约千万次，查询约10亿次。设计该系统


一个大的含有50M个URL的记录，一个小的含有500个URL的记录，找出两个记录里相同的URL。
ac自动机
在这些方法之前，可不可以先将两个文件都分割一下，然后再一一对应着地求交集？

已知一个字串由GBK汉字和ascii编码的数字和字母混合组成，编写c语言函数实现从中去掉所有ascii编码的字母和数字(包括大小写)，要求在原字符串上返回结果。
例如： “http://hi.baidu.com/mianshiti 是讨论IT面试题的博客” 会变成 “://../ 是讨论面试题的博客”。
注：函数接口为：int filter_ascii(char* gbk)；汉字的GBK编码范围是0x8140-0xFEFE。

序列Seq=[a,b,…z,aa,ab…az,ba,bb,…bz,…,za,zb,…zz,aaa,…] 类似与excel的排列，任意给出一个字符串s=[a-z]+(由a-z字符组成的任意长度字符串），请问s是序列Seq的第几个。

写一段程序，找出数组中第k大小的数，输出数所在的位置。例如{2，4，3，4，7}中，第一大的数是7，位置在4。第二大、第三大的数都是4，位置在1、3随便输出哪一个均可。
1.我们还可以通过二分的思想，找到第k大的数字，而不必对整个数组排序。
从数组中随机选一个数t，通过让这个数和其它数比较，我们可以将整个数组分成了两部分并且满足，{x, xx, ..., t} < {y, yy, ...}。
在将数组分成两个数组的过程中，我们还可以记录每个子数组的大小。这样我们就可以确定第k大的数字在哪个子数组中。
然后我们继续对包含第k大数字的子数组进行同样的划分，直到找到第k大的数字为止。
平均来说，由于每次划分都会使子数组缩小到原来1/2，所以整个过程的复杂度为O(N)。
2.如果对空间没要求的话，可以建立一个k大小的数组B，遍历原数组插入到B中，相当于是个插入排序，只不过只保留前k个元素，时间是O(n*logk)，也就是O(n)
3.如果都是整数范围又小优先用hash数组
4.用红黑树 logN

英文拼写纠错

1000w个关键词，每个的长度为1-255字节，去重后不超过300w个,找出最热门的10个，使用的内存不超过1G
有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。如何按照query的频度排序？
1）读取10个文件，按照hash(query)%10的结果将query写到对应的文件中。
     这样我们就有了10个大小约为1G的文件。任意一个query只会出现在某个文件中。
2）对于1）中获得的10个文件，分别进行如下操作
     - 利用hash_map（query，query_count）来统计每个query出现的次数。
     - 利用堆排序算法对query按照出现次数进行排序。
     - 将排序好的query输出的文件中。
    这样我们就获得了10个文件，每个文件中都是按频率排序好的query。
3）对2）中获得的10个文件进行归并排序，并将最终结果输出到文件中。

注：如果内存比较小，在第1）步中可以增加文件数。

在100w个数中找最大的前100个数

n个空间（其中n<1M），存放a到a+n-1的数，位置随机且数字不重复，a为正且未知。现在第一个空间的数被误设置为-1。已经知道被修改的数不是最小的。请找出被修改的数字是多少。
例如：n=6，a=2，原始的串为5, 3, 7, 6, 2, 4。现在被别人修改为-1, 3, 7, 6, 2, 4。现在希望找到5。
total - sub_total
求和可能会导致溢出的，可以做一个抑或操作，一个数x，x^x=0,x^0=x,所以修改后的第二个空间到最后一个空间所有数抑或一遍，然后再抑或a到a+n-1,相当于(3^3)^(7^7)...(4^4)^5=5^0=5,结果就是被修改的数字
从第二个元素开始反复将该元素交换到合适的位置上(value-a),待所有元素都放在合适的位置上后再检查-1的位置即可。 

找树中两个节点的最近公共祖先
最近公共祖先有一个好的特点，设A，B两个节点的过最近公共祖先C，那么A,B必然在C的两支上。
const Node* LCA(const Node* root, const Node* a, const Node* b) 
{
    if(root == a) return a;
    if(root == b) return b;
    if(root == NULL) return NULL;
    const Node* llca = LCA(root->left, a, b);
    const Node* rlca = LCA(root->left, a, b);
    if(llca && rlca) return root; // 这就是我们要找的节点。

    // 否则返回我们得到的不是零的节点
    if(llca) return llca;
    return rlca; 
}
如果树的深度不超过32，可以使用int32的一个整数。 
给这棵树从上到下赋值： 
第1层1； 
第2层2、3； 
第3层4、5、6、7 
依次类推，我们可以从数字推算出当前节点所在的位置：深度和左右。 
找到2个节点所对应的数字，深的节点向上冒，直到和浅的节点深度相同，这时，如果相同，那么就是这个点，如果不同，则2个节点同时向上冒，直到相遇位置，找到那个交点。

给定二叉树中的两个节点n1, n2(假定n1<n2), 其最近的公共祖先节点的值n应该满足 n1<n<n2，所以我们可以前序遍历二叉搜索树，当发现一个节点的值在n1和n2之间时，则此节点为所求节点。如果节点的值大于n1和n2，则所求节点在当前节点的左子树；否则在右子树。


请实现两棵树是否相等的比较，相等返回1，否则返回其他值，并说明算法复杂度
1. 首先将两棵树转换为最小表示。 
2. 再按照子节点比较两棵树的大小，这时在比较过程中就可以不用考虑子节点可能的对应关系了。 

你们的方法都不够好，递归就不说了，大数据量根本顶不住，树的最小表示虽然好一些，但是并没有从根本上解决递归的问题，六楼字符串的思路也不错，但是时间复杂度仍然不行，因为字符串查找匹配最坏情况下需要n2的时间，比方说两棵树的左右子节点完全颠倒的情况。 
所以得用我的方法：深度优先遍历第一棵树（用栈可以避免递归），将输出的结果保存到一个队列中，然后深度优先遍历第二颗树：根节点入栈，根节点出栈，队列队首元素出队，如果两者不同则肯定不一样，退出，否则查找根节点的左右子节点，队列队首元素出队，看队首元素是否于左右子节点相同，如果都不同，则不一样，退出，否则不同的那个先入栈，相同的那个不入栈，直接找它的子节点，队列队首元素出队，看队首元素是否于左右子节点相同，。。。。。。。如此循环，直到栈空，遍历完毕则两树相同。 
时间复杂度是2*N

对任意输入的正整数N，求N!的尾部连续0的个数，并指出计算复杂度。如：18！＝6402373705728000，尾部连续0的个数是3。（不用考虑数值超出计算机整数界限的问题）
容易理解，题目等价于求因子2和因子5出现的次数。
对于因子2来说，数字2，4，6，8，10....2n...中存在因子2，这样就获得了 N/2 （其中N/2只取结果的整数部分）个因子2。这些数字去除因子2后，变成1，2，3....N/2，又可以提取N/4个因子2....这样一直到只剩下1个数（1）。所以N！中总共可以获得N/2 + N/4 + N/8 +....个因子2。
同理，N！中可以获得N/5 + N/25 + ... 个因子5。
尾部连续0的个数就是因子2和因子5较少的那个。
对于题目中的例子，18！中包含9+4+2+1个因子2，包含3个因子5。所以尾部有3个连续0。
计算的复杂度为O(logN)。

实现一个函数，对一个正整数n，算得到1需要的最少操作次数。
操作规则为：如果n为偶数，将其除以2；如果n为奇数，可以加1或减1；一直处理下去。
例子：
func(7) = 4，可以证明最少需要4次运算
n = 7
n-1 6
n/2 3
n-1 2
n/2 1
要求：实现函数(实现尽可能高效) int func(unsign int n)；n为输入，返回最小的运算次数。
给出思路(文字描述)，完成代码，并分析你算法的时间复杂度。

提高效率的关键就是尽量能够执行除2操作。不知下面方法是否正确：
int func(unsigned int n)
{
    int cnt = 0;
    while(n > 3) {
        cnt++;
        if (3 == (n&3))
            n++;
        else if (n&1)
            n--;
        else
            n >>= 1;
    }
    if (3 == n)
        return cnt + 2;
    else if (2 == n)
        return cnt + 1;
    else
        return cnt;
}
除2相当于>>操作，总的次数为n二进制表示的位数
奇数时减1，相当于判断二进制最后一位为1的次数，由于运算过程一直在右移，也就相当于在数n二进制表示的1的个数
由于最后结果为1是既不需要移位也不需要减1，因此可以减少2次操作

得到公式
f(n) = N + M - 2
N表示将n表示为2进制时的位数
M表示将n表示为2进制时里面1的个数


例如
5
二进制为
101
N=3, M=2
f(5) = 3+2-2=3


7
111
N=3,M=3
f(7)=3+3-2=4


30
11110
N=5,M=4
f(30)=5+4-2=7


15
1111
N=4,M=4
f(15)=4+4-2=6


若2^i>n并且2^(i-1)<=n,则N=i
int f(n)
{
int N = 0;
int M = 0;
int i=1;
for (; i<=n; i<<1)
{
   if (i&n)
   M++;


   N++;
}
return N+M-2;
}
用动态规划的思想，减少递归重复计算的次数，我觉得这样复杂度会低一些

int returncycle(int n){
    int *a = malloc((n+1)*sizeof(int));
    int i,j;
    a[0]=0;
    a[1]=0;
    a[2]=1;
    for(i=3;i<=n;i++){
        if(i%2==1)
            a[i]=a[i-1]+1;
        else
            a[i]=a[i/2]+1;

    }
    j = a[n];
    free(a);
    return j;

}
怎么说你们好呢，这么简单的问题纠结成这样，看我的： 
思路：对于二进制来说，题目给出的操作实际上就是加减一或者右移一位，如果最后一位为0则右移，如果最后一位为1则加或者减1，使之变为0然后右移。至于到底是加一还是减一，则要看前面一位是否为1。对于倒数第二位为0的情况只需要减一然后右移即可，对于倒数第二位为1的情况，则加一，因为加一不仅使得最后一位为0，也使得倒数第二位为0，接下来只需要右移就可以了。这样设计的原因是：对于最后一位为1，且倒数第二位为1的情况，如果减一然后右移，两次操作只消掉一位，而如果是加一然后右移呢，虽然也是两次操作只消掉一位，但是由于进位使得倒数第二位也变为了0，所以节省了一次操作，有人可能会想到这样会使数字整体多出一位，我认为多出这一位虽然会使得前面节省下来的一次操作没有意义，但是如果倒数第三位也是1呢，就会节省两次，所以说，当倒数第二位为1的时候，用我的前面说的策略可能会不节省操作，但是也可能会节省操作，最起码不会增加操作，具体节省还是不节省，还是得看低位连续1的个数，比方说1111，按照我的策略只需要5次操作（加一、右移*4），而如果减一，则会有六次操作（减一、右移、减一、右移

给定函数d(n) = n + n的各位之和，n为正整数，如 d(78) = 78+7+8=93。 这样这个函数可以看成一个生成器，如93可以看成由78生成。
定义数A：数A找不到一个数B可以由d(B)=A，即A不能由其他数生成。现在要写程序，找出1至10000里的所有符合数A定义的数


三个警察和三个囚徒共同旅行。一条河挡住了去路，河边有一条船，但是每次只能载2人。存在如下的危险：无论在河的哪边，当囚徒人数多于警察的人数时，将有警察被囚徒杀死。
问题：请问如何确定渡河方案，才能保证6人安全无损的过河。
警察囚徒过去，警察回来
囚徒囚徒过去，囚徒回来
警察警察过去，警察囚徒回来
警察警察过去，囚徒回来
囚徒囚徒过去，囚徒回来
囚徒囚徒过去


要求设计一个DNS的Cache结构，要求能够满足每秒5000以上的查询，满足IP数据的快速插入，查询的速度要快。（题目还给出了一系列的数据，比如：站点数总共为5000万，IP地址有1000万，等等）
DNS服务器实现域名到IP地址的转换。 

每个域名的平均长度为25个字节（估计值），每个IP为4个字节，所以Cache的每个条目需要大概30个字节。
总共50M个条目，所以需要1.5G个字节的空间。可以放置在内存中。（考虑到每秒5000次操作的限制，也只能放在内存中。） 

可以考虑的数据结构包括hash_map，字典树，红黑树等等。

给定一个字符串的集合，格式如：{aaa bbb ccc}， {bbb ddd}，{eee fff}，{ggg}，{ddd hhh}要求将其中交集不为空的集合合并，要求合并完成后的集合之间无交集，例如上例应输出{aaa bbb ccc ddd hhh}，{eee fff}， {ggg}。
（1）请描述你解决这个问题的思路；
（2）请给出主要的处理流程，算法，以及算法的复杂度
（3）请描述可能的改进。
用并查集做，同一个集合里的元素合并到一颗树上，最后看有多少颗树就行了，每颗树上的元素就是一个结果集，O(n)的复杂度，很简单的
哈希表加并差集，先用哈希把字符串转成整数，转换的时候就用并差集来操作求集合的并，建立哈希O(n) 哈希查询O(1) 并差集复杂度不好估计，大概为a*O(1)
并查集或者是深度搜索
其实这个问题有两个子问题：
①查找集合的交集：用hash散列，也就是LZ的方法;时间复杂度为O(n)
②将有交集的集合合并:并查集。也是LZ给的方法,由于并查集的效率很高，基本上find,union可以是O(1)。这样基本上就是遍历一下散列表。时间复杂度为O(m);(m为散列表的长度)
综合起来，时间复杂度应该为O(m).


有2k块芯片，已知好芯片比坏芯片多。请设计算法从其中找出一片好芯片，说明你所用的比较次数上限。
其中好芯片和其它芯片比较时，能正确给出另一块芯片是好还是坏。坏芯片和其它芯片比较时，会随机的给出好或是坏。
两块芯片比较，结果可能是（好好），（好坏），（坏坏）。如果结果是（好好），则可能是两块好芯片，也可能是两块坏芯片。如果结果是（好坏）或者（坏坏），则两块芯片中至少有一块是坏的。
1. 将2k块芯片分成k组，每组两个比较。如果结果是（好坏）或者（坏坏），则把两块芯片都去掉；如果结果是（好好），则任选其中一块去掉。可以保证剩下好芯片比坏芯片多。
2. 如果剩下1个或2个，则可以判断这些是好芯片。
3. 如果剩下偶数个，则回到第1步。
4. 如果剩下奇数个，则任取一个和其它所有芯片比较，如果有一半将此芯片判为好芯片，则说明此芯片为好芯片，测试结束；否则，说明此芯片为坏芯片，去掉此芯片（当然还可以去掉将此芯片判为好芯片的芯片）后总芯片数为偶数，回到第1步。

考虑最坏的情况：
第1次，进行k次比较，可以使芯片数减半；
第2次，k为奇数，则需要进行k-1次比较，使芯片数减为k-1。
第3次，通过（k-1）/ 2次比较可以使芯片数减为 （k-1）/2。
第4次，（k-1）/2为奇数，需要（k-1）/2 - 1次比较
第5次，需要 （（k-1）/2 - 1）/2次比较
..
所以最多4k次比较。

海量日志数据，提取出某日访问百度次数最多的那个IP
IP地址最多有2^32=4G种取值可能，所以不能完全加载到内存中。 
可以考虑分而治之的策略，按照IP地址的hash(IP)%1024值，将海量日志存储到1024个小文件中。每个小文件最多包含4M个IP地址。 
对于每个小文件，可以构建一个IP作为key，出现次数作为value的hash_map，并记录当前出现次数最多的1个IP地址。 
有了1024个小文件中的出现次数最多的IP，我们就可以轻松得到总体上出现次数最多的IP。

设总共有N个IP，分别是IP1,IP2,...,IPN；这些IP中存在重复，求出重复次数最多的IP；
1、如果知道这些IP出现的次数，假设次数存放在数组S[N]中，则只需要遍历一次这个数组即可得到次数最多的IP，时间复杂度O(N)；
2、如何求出这些IP出现的次数，最直接的方法是对他们进行排序再进行次数统计，一般情况下，最快的排序方法时间复杂度是O(NlogN)，空间复杂度O(1)；可否利用hash表或者计数排序等方法进行次数统计呢？由于IP地址的范围是0~2^32-1，所以可以开辟一个数组进行计数排序，时间复杂度O(N)，空间复杂度O(2^32)；由于实际使用的IP地址范围会小很多，所以空间复杂度会更小；如果内存有限，则可以对IP地址进行分段统计次数，如分成0~2^30-1，2^30~2^31-1，2^31~2^31+2^30-1，2^31+2^30~2^32-1等，假设当前统计的是0~2^30-1的IP，则每次读入IP，如果属于这个段的，就统计，否则将这个IP存放到文件中；统计完0~2^30的IP后，找出这个段中次数最多的IP，继续统计剩下的IP；这种方法，需要多次读入文件，时间复杂度O(N)，空间复杂度是内存的最大值；
3、有没有更好的方法呢？只需要找出次数最多的IP，没必要排序，也没必要对每个IP都统计出现的次数，如何减少这些没必要


现在有一个数组，已知一个数出现的次数超过了一半，请用O(n)的复杂度的算法找出这个数
数组大小n为奇数时，任取一个与其它比较，若一半以上（含）相同，则这个数即为所求，否则删掉这个数。
n为偶数时，两两配对比较，相同留一个，不同全删除。
反复以上过程，最后剩的一个即为所求。
复杂度也是O(n)
使用两个变量A和B，其中A存储某个数组中的数，B用来计数。开始时将B初始化为0。 
遍历数组，如果B=0，则令A等于当前数，令B等于1；如果当前数与A相同，则B=B+1；如果当前数与A不同，则令B=B-1。遍历结束时，A中的数就是要找的数。 
这个算法的时间复杂度是O(n)，空间复杂度为O(1)。
编程之美《寻找水王》，现在问题来了，如果有三个数出现的次数都超过1/4，怎么找出这三个数来？
基本上很容易扩展到K个ID，其中每个ID出现的次数都超过N/(K+1)次的情况，其中，N为ID列表长度

给定一个单词a，如果通过交换单词中字母的顺序可以得到另外的单词b，那么定义b是a的兄弟单词。现在给定一个字典，用户输入一个单词，如何根据字典找出这个单词有多少个兄弟单词？
首先定义一个key，使得兄弟单词有相同的key，不是兄弟的单词有不同的key。例如，将单词按字母从小到大重新排序后作为其key，比如bad的key为abd，good的key为dgoo。 
使用链表将所有兄弟单词串在一起，hash_map的key为单词的key，value为链表的起始地址。 
开始时，先遍历字典，将每个单词都按照key加入到对应的链表当中。当需要找兄弟单词时，只需求取这个单词的key，然后到hash_map中找到对应的链表即可。 
这样创建hash_map时时间复杂度为O(n)，查找兄弟单词时时间复杂度是O(1)。

LZ方法中求key的时候需要对所有的单词排序，这样导致O(n)的系数很大，其实可以直接定义一个映射关系，每个字母对应一个素数{a:2,b:3,c:5...z:素数} 将每个单词所有字母相乘，可以确保兄弟单词的key相同，非兄弟单词的key不同


给定a、b两个文件，各存放50亿个url，每个url各占用64字节，内存限制是4G，如何找出a、b文件共同的url？
可以估计每个文件的大小为5G*64=300G，远大于4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 
遍历文件a，对每个url求取hash(url)%1000，然后根据所得值将url分别存储到1000个小文件（设为a0,a1,...a999）当中。这样每个小文件的大小约为300M。遍历文件b，采取和a相同的方法将url分别存储到1000个小文件(b0,b1....b999)中。这样处理后，所有可能相同的url都在对应的小文件(a0 vs b0, a1 vs b1....a999 vs b999)当中，不对应的小文件（比如a0 vs b99）不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 
比如对于a0 vs b0，我们可以遍历a0，将其中的url存储到hash_map当中。然后遍历b0，如果url在hash_map中，则说明此url在a和b中同时存在，保存到文件中即可。 
如果分成的小文件不均匀，导致有些小文件太大（比如大于2G），可以考虑将这些太大的小文件再按类似的方法分成小小文件即可。

用Bloom filter是可以的，前提是保证一定的误差率
发现你好像很多题目都用hash。考虑极限情况，假如a中50亿条全为1个url，如何做？


象搜索的输入信息是一个字符串，统计300万输入信息中的最热门的前10条，我们每次输入的一个字符串为不超过255byte，内存使用只有1G。请描述思想，写出算法（c语言），空间和时间复杂度。
300万个字符串最多（假设没有重复，都是最大长度）占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理.
e树，节点（不仅仅是叶子节点）额外存储count属性，记录该字符串出现的次数，这样时间复杂度是 O(n*le)，空间复杂度会节省很多，le表示字符串的平均长度。


